"""
Cantine meal prediction pipeline
Author: ChatGPT
Date: 2025-10-14

This script implements the full pipeline described by the user:
- Predict nb_entrees_HO (employees entering company) using data from 2022-05-05 to 2024-09-26
- Predict nb_entrees_cantine using the same historical data, and using predicted nb_entrees_HO for the future period
- Preprocessing: map jour_alpha -> day_week, add many time features, standardscaler for numeric, onehot for categoricals
- No lag features
- Train/test preprocessing done separately to avoid leakage
- Models: Ridge, Lasso, RandomForest, HistGradientBoosting, simple PyTorch NN (tensor conversion shown)
- TimeSeriesSplit CV and RandomizedSearchCV hyperparameter tuning
- Save best models and produce CSV predictions for 2024-09-29 to 2024-12-31

Notes:
- Requires: pandas, numpy, scikit-learn, matplotlib, joblib, torch (optional). xgboost is not required but can be added.
- Adjust file paths and environment as needed.

Usage:
- Put your historical CSV in the same folder as the script named 'historical_cantine.csv' OR modify the path.
- Run: python cantine_meal_prediction_pipeline.py
- Output: 'predictions_2024-09-29_to_2024-12-31.csv' and saved models in ./models/
"""

import os
import warnings
from datetime import datetime

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.base import clone
import joblib

# Optional PyTorch model
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    TORCH_AVAILABLE = True
except Exception:
    TORCH_AVAILABLE = False

warnings.filterwarnings('ignore')

# ---------------------------
# Config
# ---------------------------
DATA_PATH = 'historical_cantine.csv'  # update if needed
OUTPUT_PRED_CSV = 'predictions_2024-09-29_to_2024-12-31.csv'
MODELS_DIR = 'models'
os.makedirs(MODELS_DIR, exist_ok=True)
RANDOM_STATE = 42

# Dates
TRAIN_END = pd.to_datetime('2024-09-26')  # inclusive
PRED_START = pd.to_datetime('2024-09-29')
PRED_END = pd.to_datetime('2024-12-31')

# ---------------------------
# Helper functions
# ---------------------------

def load_data(path):
    df = pd.read_csv(path)
    # Ensure date column
    if 'date' not in df.columns:
        raise ValueError("No 'date' column in CSV")
    df['date'] = pd.to_datetime(df['date'], dayfirst=True, errors='coerce')
    if df['date'].isna().any():
        raise ValueError('Some dates could not be parsed. Check date format (dayfirst assumed).')
    return df.sort_values('date').reset_index(drop=True)


def map_jour_alpha(df, col='jour_alpha'):
    # Map French weekday names to numbers (0=Monday..6=Sunday) or as requested numeric sequence
    mapping = {
        'lundi': 0, 'mardi': 1, 'mercredi': 2, 'jeudi': 3, 'vendredi': 4, 'samedi': 5, 'dimanche': 6,
        'Lundi': 0, 'Mardi': 1, 'Mercredi': 2, 'Jeudi': 3, 'Vendredi': 4, 'Samedi': 5, 'Dimanche': 6
    }
    if col in df.columns:
        df['day_week'] = df[col].map(mapping)
    else:
        # fallback: derive from date
        df['day_week'] = df['date'].dt.dayofweek
    return df


def add_time_features(df):
    # assume df has 'date' and day_week
    df = df.copy()
    df['day'] = df['date'].dt.day
    df['month'] = df['date'].dt.month
    df['year'] = df['date'].dt.year
    df['day_of_week'] = df['date'].dt.dayofweek  # 0=Mon
    df['week_of_year'] = df['date'].dt.isocalendar().week.astype(int)
    df['day_of_year'] = df['date'].dt.dayofyear
    df['is_weekend'] = df['day_of_week'].isin([5,6]).astype(int)
    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)
    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)
    df['quarter'] = df['date'].dt.quarter
    # seasonal cyclical features
    df['sin_doy'] = np.sin(2 * np.pi * df['day_of_year'] / 365.25)
    df['cos_doy'] = np.cos(2 * np.pi * df['day_of_year'] / 365.25)
    # weekly cyclical
    df['sin_woy'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
    df['cos_woy'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
    return df


def get_feature_lists(df):
    # numeric features as requested
    numeric_feats = ['temperature', 'vitesse du vent']
    # The rest categorical except targets and date
    excluded = set(['date', 'nb_entrees_HO', 'nb_entrees_cantine'])
    excluded.update(numeric_feats)
    # also the original jour_alpha
    excluded.add('jour_alpha')
    # include engineered numerical features we created
    engineered_numerics = ['day', 'day_of_week', 'week_of_year', 'day_of_year', 'is_weekend',
                           'is_month_start', 'is_month_end', 'quarter', 'sin_doy', 'cos_doy', 'sin_woy', 'cos_woy', 'day_week']
    # categorical: all columns not in excluded plus certain booleans that are categorical
    categorical_candidates = [c for c in df.columns if c not in excluded and c not in engineered_numerics]
    # Force boolean-ish columns to be categorical
    # Return lists
    return numeric_feats + engineered_numerics, categorical_candidates


def preprocess_fit_transform(train_df, numeric_features, categorical_features):
    # Create ColumnTransformer and fit on train
    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
    categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))])
    preprocessor = ColumnTransformer(transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ], remainder='drop')
    X_train = train_df[numeric_features + categorical_features]
    preprocessor.fit(X_train)
    X_train_trans = preprocessor.transform(X_train)
    # Get feature names for the transformed array
    cat_columns = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)
    feature_names = numeric_features + list(engineered_feature_names() if False else cat_columns)  # placeholder; we keep numeric names followed by OHE names
    return preprocessor, X_train_trans


def engineered_feature_names():
    # currently unused
    return []


def evaluate_model(model, X_val, y_val):
    preds = model.predict(X_val)
    mse = mean_squared_error(y_val, preds)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_val, preds)
    r2 = r2_score(y_val, preds)
    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}

# ---------------------------
# Main pipeline functions
# ---------------------------

def prepare_data(df):
    df = df.copy()
    # map jour_alpha -> day_week
    df = map_jour_alpha(df, col='jour_alpha')
    # add time features
    df = add_time_features(df)
    return df


def split_train_pred_dfs(df):
    df_hist = df[df['date'] <= TRAIN_END].copy()
    df_future = df[(df['date'] >= PRED_START) & (df['date'] <= PRED_END)].copy()
    # If the future rows do not exist in your CSV (likely), generate future dataframe with required columns
    if df_future.empty:
        # build future date range and fill NA for features that are unknown (we keep categorical columns as zeros or most common)
        future_dates = pd.date_range(PRED_START, PRED_END, freq='D')
        df_future = pd.DataFrame({'date': future_dates})
        # copy columns from df_hist and set defaults
        for c in df.columns:
            if c == 'date':
                continue
            df_future[c] = np.nan
        # Try to fill features that can be derived from date
        df_future = map_jour_alpha(df_future, col='jour_alpha')
        df_future = add_time_features(df_future)
    return df_hist.reset_index(drop=True), df_future.reset_index(drop=True)


def build_preprocessor(numeric_features, categorical_features):
    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])
    categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))])
    preprocessor = ColumnTransformer(transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ], remainder='drop')
    return preprocessor


def get_cv():
    # Use TimeSeriesSplit for time-aware CV
    tscv = TimeSeriesSplit(n_splits=5)
    return tscv


def train_and_tune(X, y, preprocessor, model_name='ridge'):
    # Returns best estimator (pipeline), cv results
    if model_name == 'ridge':
        base = Ridge(random_state=RANDOM_STATE)
        param_dist = {'model__alpha': [0.1, 1, 10, 50, 100]}
    elif model_name == 'lasso':
        base = Lasso(random_state=RANDOM_STATE, max_iter=5000)
        param_dist = {'model__alpha': [0.0001, 0.001, 0.01, 0.1, 1]}
    elif model_name == 'rf':
        base = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)
        param_dist = {
            'model__n_estimators': [100, 200],
            'model__max_depth': [5, 10, None],
            'model__min_samples_leaf': [1, 3, 5]
        }
    elif model_name == 'hgb':
        base = HistGradientBoostingRegressor(random_state=RANDOM_STATE)
        param_dist = {'model__max_iter': [100, 200], 'model__max_depth': [3, 10, None]}
    elif model_name == 'torch' and TORCH_AVAILABLE:
        # we'll wrap a small torch model in a sklearn-like API for simplicity
        return train_torch_model(X, y, preprocessor)
    else:
        raise ValueError('Unknown model')

    pipe = Pipeline(steps=[('preprocessor', preprocessor), ('model', base)])
    tscv = get_cv()
    search = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=6, cv=tscv,
                                scoring='neg_root_mean_squared_error', random_state=RANDOM_STATE, n_jobs=-1, verbose=1)
    search.fit(X, y)
    return search.best_estimator_, search

# ---------------------------
# Optional PyTorch wrapper
# ---------------------------

def train_torch_model(X, y, preprocessor, epochs=50, batch_size=32):
    # Fit preprocessor on X
    X_trans = preprocessor.fit_transform(X)
    X_arr = np.array(X_trans, dtype=np.float32)
    y_arr = np.array(y, dtype=np.float32).reshape(-1, 1)
    # simple NN
    input_dim = X_arr.shape[1]
    model = nn.Sequential(
        nn.Linear(input_dim, 64),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Linear(64, 32),
        nn.ReLU(),
        nn.Linear(32, 1)
    )
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    loss_fn = nn.MSELoss()
    dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_arr), torch.from_numpy(y_arr))
    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)
    model.train()
    for epoch in range(epochs):
        epoch_loss = 0.0
        for xb, yb in loader:
            optimizer.zero_grad()
            preds = model(xb)
            loss = loss_fn(preds, yb)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item() * xb.size(0)
        # print every 10 epochs
        if (epoch + 1) % 10 == 0:
            print(f'Epoch {epoch+1}/{epochs} loss={epoch_loss/len(dataset):.4f}')
    # Wrap preprocessor + torch model for predict
    class TorchWrapper:
        def __init__(self, prep, model):
            self.prep = prep
            self.model = model

        def predict(self, X_new):
            X_t = self.prep.transform(X_new)
            X_t = torch.from_numpy(np.array(X_t, dtype=np.float32))
            self.model.eval()
            with torch.no_grad():
                preds = self.model(X_t).numpy().reshape(-1)
            return preds

    wrapper = TorchWrapper(preprocessor, model)
    return wrapper, None

# ---------------------------
# Training workflow
# ---------------------------

def run_first_stage(df_hist, df_future):
    """
    Stage 1: target = nb_entrees_HO
    """
    print('Stage 1: Predicting nb_entrees_HO')
    # drop rows missing target
    df = df_hist.dropna(subset=['nb_entrees_HO']).copy()

    # feature lists
    numeric_feats, categorical_feats = get_feature_lists(df)
    # ensure numeric feats exist
    numeric_feats = [c for c in numeric_feats if c in df.columns]
    categorical_feats = [c for c in categorical_feats if c in df.columns]

    preprocessor = build_preprocessor(numeric_feats, categorical_feats)

    X = df[numeric_feats + categorical_feats]
    y = df['nb_entrees_HO']

    # Train/test splitting for evaluation: keep last 30 days as holdout from historical
    holdout_days = 30
    holdout_start_date = df['date'].max() - pd.Timedelta(days=holdout_days)
    train_mask = df['date'] <= holdout_start_date
    X_train = X[train_mask]
    y_train = y[train_mask]
    X_hold = X[~train_mask]
    y_hold = y[~train_mask]

    print('Training set:', X_train.shape, 'Holdout:', X_hold.shape)

    models_to_try = ['ridge', 'lasso', 'rf', 'hgb']
    if TORCH_AVAILABLE:
        models_to_try.append('torch')

    results = {}
    best_model = None
    best_rmse = np.inf

    for mname in models_to_try:
        try:
            print('\nTraining model:', mname)
            est, search = train_and_tune(X_train, y_train, preprocessor, model_name=mname)
            # Evaluate on holdout
            X_hold_trans = X_hold  # est already contains preprocessor pipeline
            preds = est.predict(X_hold_trans)
            metrics = evaluate_model(est, X_hold_trans, y_hold)
            print(f"{mname} holdout RMSE: {metrics['rmse']:.2f}, MAE: {metrics['mae']:.2f}, R2: {metrics['r2']:.3f}")
            results[mname] = {'estimator': est, 'search': search, 'metrics': metrics}
            if metrics['rmse'] < best_rmse:
                best_rmse = metrics['rmse']
                best_model = est
        except Exception as e:
            print('Error training', mname, e)

    print('\nBest model on holdout:', type(best_model.named_steps['model']).__name__ if isinstance(best_model, Pipeline) else type(best_model).__name__, 'RMSE=', best_rmse)
    # Save best model
    joblib.dump(best_model, os.path.join(MODELS_DIR, 'best_model_nb_entrees_HO.joblib'))

    # Make predictions for future dates
    X_future = df_future[numeric_feats + categorical_feats]
    # Some categorical/numeric columns may be missing in future; fill with defaults
    for c in numeric_feats:
        if c not in X_future.columns:
            X_future[c] = 0.0
    for c in categorical_feats:
        if c not in X_future.columns:
            X_future[c] = 0
    # For any NA values, fill with median or mode from historical
    X_future = X_future.fillna(df[numeric_feats + categorical_feats].median(numeric_only=True).to_dict())
    X_future = X_future.fillna(0)

    preds_future = best_model.predict(X_future)
    df_future_preds = df_future[['date']].copy()
    df_future_preds['pred_nb_entrees_HO'] = np.round(preds_future).astype(int)

    return best_model, df_future_preds, results


def run_second_stage(df_hist, df_future, df_future_preds):
    """
    Stage 2: target = nb_entrees_cantine, include nb_entrees_HO as numeric feature for training and for future use include predicted HO
    """
    print('\nStage 2: Predicting nb_entrees_cantine')
    df = df_hist.copy()
    # ensure nb_entrees_HO is numeric and present
    if df['nb_entrees_HO'].isna().any():
        # drop rows where HO missing or fill
        df = df.dropna(subset=['nb_entrees_HO'])

    # Insert predicted nb_entrees_HO for future df
    df_future = df_future.copy()
    df_future = df_future.merge(df_future_preds[['date', 'pred_nb_entrees_HO']], on='date', how='left')
    df_future['nb_entrees_HO'] = df_future['pred_nb_entrees_HO']

    # feature lists
    numeric_feats, categorical_feats = get_feature_lists(df)
    # include nb_entrees_HO as numeric
    if 'nb_entrees_HO' not in numeric_feats:
        numeric_feats = numeric_feats + ['nb_entrees_HO']

    numeric_feats = [c for c in numeric_feats if c in df.columns]
    categorical_feats = [c for c in categorical_feats if c in df.columns]

    preprocessor = build_preprocessor(numeric_feats, categorical_feats)

    X = df[numeric_feats + categorical_feats]
    y = df['nb_entrees_cantine']

    # Holdout last 30 days
    holdout_days = 30
    holdout_start_date = df['date'].max() - pd.Timedelta(days=holdout_days)
    train_mask = df['date'] <= holdout_start_date
    X_train = X[train_mask]
    y_train = y[train_mask]
    X_hold = X[~train_mask]
    y_hold = y[~train_mask]

    print('Training set:', X_train.shape, 'Holdout:', X_hold.shape)

    models_to_try = ['ridge', 'lasso', 'rf', 'hgb']
    if TORCH_AVAILABLE:
        models_to_try.append('torch')

    results = {}
    best_model = None
    best_rmse = np.inf

    for mname in models_to_try:
        try:
            print('\nTraining model:', mname)
            est, search = train_and_tune(X_train, y_train, preprocessor, model_name=mname)
            preds = est.predict(X_hold)
            metrics = evaluate_model(est, X_hold, y_hold)
            print(f"{mname} holdout RMSE: {metrics['rmse']:.2f}, MAE: {metrics['mae']:.2f}, R2: {metrics['r2']:.3f}")
            results[mname] = {'estimator': est, 'search': search, 'metrics': metrics}
            if metrics['rmse'] < best_rmse:
                best_rmse = metrics['rmse']
                best_model = est
        except Exception as e:
            print('Error training', mname, e)

    print('\nBest model on holdout:', type(best_model.named_steps['model']).__name__ if isinstance(best_model, Pipeline) else type(best_model).__name__, 'RMSE=', best_rmse)
    joblib.dump(best_model, os.path.join(MODELS_DIR, 'best_model_nb_entrees_cantine.joblib'))

    # Predict future
    X_future = df_future[numeric_feats + categorical_feats]
    for c in numeric_feats:
        if c not in X_future.columns:
            X_future[c] = 0.0
    for c in categorical_feats:
        if c not in X_future.columns:
            X_future[c] = 0
    X_future = X_future.fillna(df[numeric_feats + categorical_feats].median(numeric_only=True).to_dict())
    X_future = X_future.fillna(0)

    preds_future = best_model.predict(X_future)
    df_future_preds_final = df_future[['date']].copy()
    df_future_preds_final['pred_nb_entrees_cantine'] = np.round(preds_future).astype(int)

    # Merge with HO predictions
    df_out = df_future_preds.merge(df_future_preds_final, on='date')
    return best_model, df_out, results

# ---------------------------
# Visualization helpers
# ---------------------------

def plot_true_vs_pred(dates, true_vals, pred_vals, title='True vs Pred'):
    plt.figure(figsize=(12,5))
    plt.plot(dates, true_vals, label='true')
    plt.plot(dates, pred_vals, label='pred')
    plt.legend()
    plt.title(title)
    plt.xlabel('date')
    plt.ylabel('count')
    plt.show()

# ---------------------------
# Main
# ---------------------------

def main():
    df = load_data(DATA_PATH)
    df = prepare_data(df)

    df_hist, df_future = split_train_pred_dfs(df)

    # Stage 1
    best_ho_model, df_future_preds_ho, results_ho = run_first_stage(df_hist, df_future)

    # Stage 2
    best_cantine_model, df_preds_final, results_cantine = run_second_stage(df_hist, df_future, df_future_preds_ho)

    # Save results to CSV
    df_preds_final = df_preds_final.sort_values('date')
    df_preds_final.to_csv(OUTPUT_PRED_CSV, index=False)
    print('Saved predictions to', OUTPUT_PRED_CSV)

    # Simple visualizations: model residuals on holdout for best models if available
    try:
        # Stage 1 residuals
        df_hist_nonnull = df_hist.dropna(subset=['nb_entrees_HO'])
        last_holdout_mask = df_hist_nonnull['date'] > (df_hist_nonnull['date'].max() - pd.Timedelta(days=30))
        X_hold = df_hist_nonnull.loc[last_holdout_mask]
        y_hold = X_hold['nb_entrees_HO']
        preds_hold = best_ho_model.predict(X_hold.drop(columns=['date', 'nb_entrees_HO', 'nb_entrees_cantine'], errors='ignore'))
        plot_true_vs_pred(X_hold['date'], y_hold, preds_hold, title='Stage1 Holdout True vs Pred')
    except Exception as e:
        print('Could not plot stage1 holdout:', e)

    try:
        # Stage 2 residuals
        df_hist_nonnull2 = df_hist.dropna(subset=['nb_entrees_cantine', 'nb_entrees_HO'])
        last_holdout_mask2 = df_hist_nonnull2['date'] > (df_hist_nonnull2['date'].max() - pd.Timedelta(days=30))
        X_hold2 = df_hist_nonnull2.loc[last_holdout_mask2]
        y_hold2 = X_hold2['nb_entrees_cantine']
        preds_hold2 = best_cantine_model.predict(X_hold2.drop(columns=['date', 'nb_entrees_HO', 'nb_entrees_cantine'], errors='ignore'))
        plot_true_vs_pred(X_hold2['date'], y_hold2, preds_hold2, title='Stage2 Holdout True vs Pred')
    except Exception as e:
        print('Could not plot stage2 holdout:', e)

    print('\nDone.')

if __name__ == '__main__':
    main()
